# AI PR Code Reviewer ğŸ¤–

> **ColorStack Winter Hackathon 2025**  
> An AI-powered code review assistant that provides instant, comprehensive feedback on GitHub Pull Requests.

---

## ğŸ’¡ Inspiration

Code reviews are essential but slow. As developers, we've all experienced:
- **The bottleneck**: Waiting days for senior engineers to review PRs while deadlines loom
- **The overwhelm**: Reviewing massive 50+ file PRs and missing critical security vulnerabilities
- **The inconsistency**: Different reviewers catching different things, leading to gaps in quality

We wanted to **democratize access to senior-level code review feedback**. Not every team has access to seasoned engineers, and even when they do, those engineers shouldn't spend hours on repetitive pattern-matching. We envisioned an AI assistant that could:
- Provide instant, comprehensive feedback 24/7
- Catch security vulnerabilities, performance issues, and code smells
- Generate actionable test plans
- Free up human reviewers to focus on architecture and business logic

The goal wasn't to replace human code reviewâ€”it was to **augment it**, making every developer more productive and every codebase more secure.

---

## ğŸ¯ What it does

Our platform accepts a GitHub Pull Request URL and returns a comprehensive AI-powered code review featuring:

### Core Features
âœ… **PR Summary** - Concise "what changed" and "why it changed" analysis with key files highlighted  
âœ… **Prioritized Findings** - Security vulnerabilities, performance bottlenecks, and maintainability issues ranked by severity  
âœ… **Risk Matrix** - Four-dimensional risk assessment (Security, Performance, Breaking Changes, Maintainability)  
âœ… **Test Plan Generator** - Suggested unit tests, integration tests, and edge cases tailored to the changes  
âœ… **Merge Readiness Score** - 0-100 score with clear blockers identified  
âœ… **GitHub Integration** - Post reviews directly as PR comments (optional)  
âœ… **Dual Analysis Modes** - AI-powered (OpenAI GPT-4o-mini) or heuristics-based pattern matching

### User Flow
1. Developer submits a GitHub PR URL via the web interface
2. Backend fetches PR metadata, diff, and file changes from GitHub API
3. Analyzer processes changes using AI or heuristics
4. Frontend displays structured review with findings, risks, and test suggestions
5. (Optional) Developer posts the review back to GitHub as a comment

---

## ğŸš€ How we built it

### Tech Stack
**Frontend**: React + TypeScript + Vite + Tailwind CSS  
**Backend**: FastAPI (Python) + SQLAlchemy + SQLite  
**AI Layer**: OpenAI GPT-4o-mini with structured JSON output  
**Deployment**: Render (backend) + Vercel (frontend)  
**Version Control**: Git with feature branches merged to main

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     HTTPS     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   FastAPI    â”‚
â”‚  (Vite/TS)  â”‚               â”‚   Backend    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼                â–¼                â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  GitHub  â”‚    â”‚  OpenAI  â”‚    â”‚ SQLite   â”‚
              â”‚   API    â”‚    â”‚   API    â”‚    â”‚   DB     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Implementation Details

**1. GitHub Service Layer**
- Parses PR URLs using regex to extract `owner/repo/number`
- Fetches PR metadata, diffs, files, and commits via GitHub REST API
- Handles redirects and rate limits with `httpx` async client
- Supports OAuth for private repos (Phase 2)

**2. Dual-Mode Analyzer**
- **AI Mode** (with OpenAI key): 
  - Sends PR context + diff to GPT-4o-mini with strict JSON schema
  - Validates output with Pydantic models
  - Retry logic for malformed JSON responses
- **Heuristics Mode** (fallback):
  - Pattern matching with 20+ security/performance regex patterns
  - Confidence scoring based on match quality
  - Generic test suggestions based on file types

**3. Scoring Algorithm**
We calculate merge readiness using weighted penalties:

$$\text{Score} = \max(0, 100 - 30n_c - 15n_h - 2n_t)$$

Where:
- $n_c$ = critical findings
- $n_h$ = high-severity findings
- $n_t$ = total findings

This ensures critical issues have exponential impact on the score.

**4. CORS Configuration**
Production deployment required flexible CORS handling:
- Parse `ALLOWED_ORIGINS` from environment as CSV
- Support `ALLOWED_ORIGIN_REGEX` for wildcard subdomain matching (Vercel)
- Automatically disable credentials when using wildcard origins

---

## ğŸ˜… Challenges we ran into

### 1. **CORS Preflight Hell** ğŸ”¥
**Problem**: Frontend on Vercel getting `400 Bad Request` on OPTIONS requests  
**Root Cause**: Backend only allowed `localhost:3000` and `localhost:5173`, but Vercel assigned port `5175`  
**Solution**: 
- Made `ALLOWED_ORIGINS` configurable via environment variable
- Added regex pattern support for Vercel subdomains (`https://.*\.vercel\.app`)
- Implemented logic to disable credentials when using wildcard origins (CORS spec requirement)

### 2. **GitHub API Redirects**
**Problem**: Some old PR URLs returned `301 Moved Permanently` and httpx wasn't following redirects  
**Root Cause**: Default httpx client doesn't follow redirects  
**Solution**: Added `follow_redirects=True` to all httpx client instantiations

### 3. **LLM Output Validation**
**Problem**: GPT-4o-mini occasionally returned malformed JSON or skipped required fields  
**Root Cause**: Non-deterministic LLM output even with `temperature=0.3`  
**Solution**:
- Enforced `response_format={"type": "json_object"}` in OpenAI API call
- Wrapped LLM parsing in try/except with fallback to heuristics
- Validated output with strict Pydantic schemas

### 4. **Large Diff Handling**
**Problem**: PRs with 100+ files exceeded OpenAI's token limits  
**Root Cause**: Some diffs were 50,000+ characters  
**Solution**: 
- Truncate diffs to 15,000 characters with clear "truncated" marker
- Prioritize showing changed lines over context
- Consider file-by-file analysis for Phase 2

### 5. **Background Task Database Sessions**
**Problem**: SQLAlchemy sessions weren't thread-safe in FastAPI BackgroundTasks  
**Root Cause**: Reusing the request's DB session in async background function  
**Solution**: Create fresh database session inside background task using database URL

---

## ğŸ† Accomplishments that we're proud of

âœ¨ **Built a production-ready full-stack app in 1.5 weeks** - Complete with API, database, frontend, and deployment  
âœ¨ **Dual analysis modes** - Works with or without OpenAI API key (accessibility for all users)  
âœ¨ **Real GitHub integration** - Actually fetches live PR data and can post comments back  
âœ¨ **Smart scoring algorithm** - Mathematical model that prioritizes critical issues exponentially  
âœ¨ **Professional UI/UX** - Clean, intuitive interface with shadcn/ui components  
âœ¨ **Comprehensive error handling** - Graceful fallbacks and user-friendly error messages  
âœ¨ **Deployed to production** - Live on Render + Vercel, fully functional  

---

## ğŸ§  What we learned

### Technical Learnings
1. **Async Python is powerful**: Using `asyncio.gather()` to fetch PR data concurrently reduced API call latency by ~60%
2. **LLM output validation is non-negotiable**: Never trust LLM output without schema validation and retry logic
3. **CORS is more nuanced than we thought**: Wildcard origins, regex patterns, and credential handling require careful configuration
4. **Background tasks need isolated resources**: Database sessions, HTTP clients, and other stateful objects must be recreated in background contexts

### Product Learnings
1. **Heuristics matter**: Even without AI, pattern-matching provides valueâ€”don't gate features behind expensive APIs
2. **Structured output > prose**: Users want JSON they can integrate into CI/CD, not just markdown comments
3. **Confidence scores build trust**: Users need to know how certain the tool is about each finding
4. **Instant feedback matters**: Even a 10-second delay feels slowâ€”optimize for speed or show progress

### Team Learnings
1. **AI pair programming accelerates MVPs**: We shipped 12,000 lines of code with AI assistance while maintaining quality
2. **Git branch workflows prevent conflicts**: Separate `backend` and `frontend` branches merged cleanly to `main`
3. **Deploy early, deploy often**: Finding the CORS issue on day 1 of deployment saved us hours of debugging later

---

## ğŸ”® What's next for colorstackwinterhack2025-codesearch

### Near-term (Next Sprint)
ğŸš€ **CI/CD Integration** - GitHub Actions workflow to auto-review PRs on push  
ğŸš€ **Custom Rules Engine** - Support for `review_rules.yml` files in repos to enforce team-specific standards  
ğŸš€ **Diff-specific line comments** - Post comments on exact lines rather than general PR comment  
ğŸš€ **Historical tracking** - Show how a repo's review scores improve over time

### Mid-term (Next Quarter)
ğŸ”® **Multi-language deep analysis** - Language-specific linters (ESLint, Pylint, Rustfmt) integrated into findings  
ğŸ”® **GitHub App installation** - Webhook-driven auto-reviews without manual URL submission  
ğŸ”® **Team dashboard** - Analytics showing common issues, top contributors, review trends  
ğŸ”® **Slack/Discord integration** - Post review summaries to team channels

### Long-term Vision
ğŸŒŸ **Private repo support** - Full GitHub OAuth flow for authenticated analysis  
ğŸŒŸ **Multi-platform support** - GitLab, Bitbucket, Azure DevOps  
ğŸŒŸ **Fine-tuned model** - Custom LLM trained on accepted vs rejected PRs from popular repos  
ğŸŒŸ **IDE extensions** - VSCode/JetBrains plugins for pre-commit local analysis  
ğŸŒŸ **Enterprise features** - Self-hosted option, SSO, audit logs, compliance reporting

---

## ğŸ› ï¸ Getting Started

### Prerequisites
- Python 3.11+
- Node.js 18+
- GitHub account
- (Optional) OpenAI API key

### Backend Setup
```bash
cd backend
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
cp .env.example .env
# Add OPENAI_API_KEY to .env (optional)
uvicorn app.main:app --reload --port 8000
```

### Frontend Setup
```bash
cd frontend
npm install
npm run dev
```

Open http://localhost:5175 and start reviewing PRs! ğŸ‰

---

## ğŸ“š Documentation

- [Backend README](backend/README.md) - API documentation, deployment guide
- [Frontend README](frontend/README.md) - Component architecture, styling guide
- [API Docs](http://localhost:8000/docs) - Interactive Swagger UI (when running locally)

---

## ğŸ¤ Contributing

This project was built for ColorStack Winter Hackathon 2025. We welcome contributions!

1. Fork the repo
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request (and let our tool review it! ğŸ˜„)

---

## ğŸ“„ License

MIT License - see LICENSE file for details

---

## ğŸ™ Acknowledgments

- **ColorStack** for hosting an incredible hackathon
- **OpenAI** for GPT-4o-mini API
- **GitHub** for comprehensive API documentation
- **FastAPI** and **React** communities for excellent frameworks
- **shadcn/ui** for beautiful component library

---

**Built with â¤ï¸ by the ColorStack Winter Hackathon 2025 Team**

*Empowering developers with AI-powered code review, one PR at a time.*
